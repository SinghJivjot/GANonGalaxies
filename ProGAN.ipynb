{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "root_path = '/mnt/c/Users/121js/OneDrive/Desktop/TorchImages/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_transpose=False, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding) if not use_transpose else nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * kernel_size**2))**0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        nn.init.normal_(self.conv.weight), nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "class PixNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixnorm=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixNorm()\n",
    "        self.use_pn = use_pixnorm\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.pn(x) if self.pn else x\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.pn(x) if self.pn else x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3, factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.initial_block = nn.Sequential(\n",
    "            PixNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixNorm(),\n",
    "        )\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, 1, 1, 0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        \n",
    "        for i in range(len(factors) - 1):\n",
    "            conv_in_chan = int(in_channels*factors[i])\n",
    "            conv_out_chan = int(in_channels*factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_chan, conv_out_chan))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_chan, img_channels, 1, 1, 0))\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha*generated + (1-alpha)*upscaled)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial_block(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upsampled = self.rgb_layers[steps-1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upsampled, final_out)\n",
    "\n",
    "class Discrimiator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3, factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors)-1, 0, -1):\n",
    "            conv_in_chan = int(in_channels*factors[i])\n",
    "            conv_out_chan = int(in_channels*factors[i-1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_chan, conv_out_chan, use_pixnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in_chan, 1, 1, 0))\n",
    "        \n",
    "        self.final_rgb = WSConv2d(img_channels, in_channels, 1, 1, 0)\n",
    "        self.rgb_layers.append(self.final_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels+1, in_channels, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, 1, 1, 0),\n",
    "        )\n",
    "        \n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha*out + (1-alpha)*downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_stats = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_stats], dim=1)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):\n",
    "        curr_steps = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.rgb_layers[curr_steps](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "        \n",
    "        downscaled = self.leaky(self.rgb_layers[curr_steps+1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[curr_steps](out))\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for curr_step in range(curr_steps+1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[curr_step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_dim = 50\n",
    "# in_channels = 256\n",
    "# factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n",
    "# img_sizes = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "# gen = Generator(z_dim, in_channels, img_channels=3, factors=factors)\n",
    "# critic = Discrimiator(in_channels, img_channels=3, factors=factors)\n",
    "\n",
    "# for img_size in img_sizes:\n",
    "#     num_steps = int(log2(img_size/4))\n",
    "#     x = torch.randn((1, z_dim, 1, 1))\n",
    "#     z = gen(x, 0.5, steps=num_steps)\n",
    "#     assert z.shape == (1, 3, img_size, img_size)\n",
    "    \n",
    "#     out = critic(z, alpha=0.5, steps=num_steps)\n",
    "#     assert out.shape == (1, 1)\n",
    "\n",
    "#     print(f'Success at image size: {img_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_IMG_SIZE = 512\n",
    "DATASET_NAME = 'spiral_galaxies'\n",
    "CHECKPOINT_GEN = 'generator.pth'\n",
    "CHECKPOINT_CRIT = 'critic.pth'\n",
    "SAVE_IMG_FOLDER = 'results'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = True\n",
    "BATCH_SIZES = [256, 256, 256, 64, 64, 16, 8, 4, 2]\n",
    "LR = 1e-3\n",
    "IMG_SIZE = 256\n",
    "IMG_CHANNELS = 3\n",
    "IN_CHANNELS = 128\n",
    "NOISE_DIM = 128\n",
    "LAMBDA_GP = 10\n",
    "NUM_STEPS = int(log2(IMG_SIZE/4)) + 1\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [200, 200, 200, 150, 150, 150, 150, 100, 100]\n",
    "NUM_IMG_TO_SHOW = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gradient_penalty(critic, real, fake, alpha, train_step, device='cuda'):\n",
    "    B, C, H, W = real.shape\n",
    "    beta = torch.rand((B, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real*beta + fake.detach()*(1-beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "    gradient = torch.autograd.grad(\n",
    "        outputs=mixed_scores,\n",
    "        inputs=interpolated_images,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm-1)**2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename='my_checkpoint.pth.tar'):\n",
    "    print('==> Saving Checkpoint <==')\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print('==> Loading Checkpoint <==')\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def show_in_tensorboard(writer, loss_crit, loss_gen, real, fake, tensorboard_step):\n",
    "    writer.add_scalar('Loss Gen', loss_gen, global_step=tensorboard_step)\n",
    "    writer.add_scalar('Loss Critic', loss_crit, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = torchvision.utils.make_grid(fake[:NUM_IMG_TO_SHOW], nrow=NUM_IMG_TO_SHOW, normalize=True)\n",
    "        real_imgs = torchvision.utils.make_grid(real[:NUM_IMG_TO_SHOW], nrow=NUM_IMG_TO_SHOW, normalize=True)\n",
    "        writer.add_image('Fake', fake_imgs, global_step=tensorboard_step)\n",
    "        writer.add_image('Real', real_imgs, global_step=tensorboard_step)\n",
    "\n",
    "# def save_some_examples(gen, steps, gen_size, folder='progan_results'):\n",
    "#     gen.eval()\n",
    "#     alpha=1\n",
    "#     with torch.no_grad():\n",
    "#         to_save = torchvision.utils.make_grid(gen(FIXED_NOISE, alpha, steps)*0.5 + 0.5, nrow=NUM_IMG_TO_SHOW)\n",
    "#         save_image(to_save, folder+f'/img_{gen_size}x{gen_size}_epoch.png')\n",
    "#     gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(img_size):\n",
    "    transformations = transforms.Compose(\n",
    "        transforms=[\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(IMG_CHANNELS)], [0.5 for _ in range(IMG_CHANNELS)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    images = datasets.ImageFolder(root=root_path+DATASET_NAME, transform=transformations)\n",
    "    images_loader = DataLoader(dataset=images, batch_size=BATCH_SIZES[int(log2(img_size/4))], shuffle=True)\n",
    "    return images, images_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(critic, gen, loader, dataset, step, alpha, opt_crit, opt_gen, tensorboard_step, writer, scaler_crit, scaler_gen):\n",
    "    # loop = tqdm(loader, leave=True)\n",
    "    loop = loader\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        noise = torch.randn(real.shape[0], NOISE_DIM, 1, 1).to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, alpha, step)\n",
    "            crit_real = critic(real, alpha, step)\n",
    "            crit_fake = critic(fake.detach(), alpha, step)\n",
    "            gp = find_gradient_penalty(critic, real, fake, alpha, step, DEVICE)\n",
    "            loss_crit = (\n",
    "                - (torch.mean(crit_real) - torch.mean(crit_fake))\n",
    "                + LAMBDA_GP*gp\n",
    "                + (1e-3*torch.mean(crit_real**2))\n",
    "            )\n",
    "        critic.zero_grad()\n",
    "        scaler_crit.scale(loss_crit).backward()\n",
    "        scaler_crit.step(opt_crit)\n",
    "        scaler_crit.update()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        alpha += real.shape[0] / ((PROGRESSIVE_EPOCHS[step]*0.5)*len(dataset))\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % int(np.sqrt(len(loader))) == 0:\n",
    "            with torch.no_grad():\n",
    "                FIXED_NOISE = torch.randn(NUM_IMG_TO_SHOW, NOISE_DIM, 1, 1).to(DEVICE)\n",
    "                fixed_fakes = gen(FIXED_NOISE, alpha, step)*0.5 + 0.5\n",
    "                to_save = torchvision.utils.make_grid(fixed_fakes, nrow=NUM_IMG_TO_SHOW)\n",
    "                save_image(to_save, SAVE_IMG_FOLDER+f'/img_{4*2**step}x{4*2**step}_{tensorboard_step}.png')\n",
    "                show_in_tensorboard(\n",
    "                    writer,\n",
    "                    loss_crit.item(),\n",
    "                    loss_gen.item(),\n",
    "                    real.detach(),\n",
    "                    fixed_fakes.detach(),\n",
    "                    tensorboard_step\n",
    "                )\n",
    "                tensorboard_step += 1\n",
    "                \n",
    "    return alpha, tensorboard_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func():\n",
    "    gen = Generator(NOISE_DIM, IN_CHANNELS, IMG_CHANNELS).to(DEVICE)\n",
    "    critic = Discrimiator(IN_CHANNELS, IMG_CHANNELS).to(DEVICE)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LR, betas=(0, 0.99))\n",
    "    opt_crit = optim.Adam(critic.parameters(), lr=LR, betas=(0, 0.99))\n",
    "    scaler_crit, scaler_gen = torch.cuda.amp.GradScaler(), torch.cuda.amp.GradScaler()\n",
    "\n",
    "    writer = SummaryWriter('logs/progan')\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LR)\n",
    "        load_checkpoint(CHECKPOINT_CRIT, critic, opt_crit, LR)\n",
    "    \n",
    "    gen.train(), critic.train()\n",
    "\n",
    "    tensorboard_step = 1\n",
    "    step = int(log2(STARTING_IMG_SIZE/4))\n",
    "\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5\n",
    "        images, images_loader = get_loader(4*2**step)\n",
    "        print(f'Current Image size: {4*2**step}')\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            alpha, tensorboard_step = train_func(\n",
    "                critic, gen, images_loader, images, step, alpha, opt_crit, opt_gen, tensorboard_step, writer,\n",
    "                scaler_crit, scaler_gen\n",
    "                )\n",
    "            if SAVE_MODEL:\n",
    "                save_checkpoint(gen, opt_gen, CHECKPOINT_GEN)\n",
    "                save_checkpoint(critic, opt_crit, CHECKPOINT_CRIT)\n",
    "    \n",
    "        step += 1\n",
    "        if 4*2**step == STARTING_IMG_SIZE*2:\n",
    "            print('Ending')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading Checkpoint <==\n",
      "==> Loading Checkpoint <==\n",
      "Current Image size: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a89b8608ba742d4819bc64f477fc235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n",
      "==> Saving Checkpoint <==\n"
     ]
    }
   ],
   "source": [
    "main_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from 65 epoch and size=256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
